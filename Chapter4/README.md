1. Silakan selesaikan praktikum tersebut sesuai langkah-langkah sebelumnya, lalu laporkan hasilnya berupa link repository GitHub dengan nama spark-sql-big-data disertai dengan screenshot hasilnya.
2. Jelaskan masing-masing maksud kode berikut sesuai nomor kodenya pada laporan praktikum Anda!
![Screenshot](images/soal.png)
    - jawab:
    
    1.mylist dan myschema adalah dua variabel dalam Python yang digunakan untuk membuat DataFrame di PySpark. mylist adalah daftar item, sedangkan myschema adalah daftar string yang mendefinisikan nama kolom DataFrame.

    2.spark.createDataFrame adalah metode PySpark yang digunakan untuk membuat DataFrame dari data yang diberikan.

    3.parallelize adalah metode PySpark yang digunakan untuk membuat RDD dari data yang diberikan, sedangkan toDF adalah metode yang digunakan untuk mengonversi RDD menjadi DataFrame.

    4.hadoop, fs, dan put adalah perintah untuk mengunggah file ke sistem file Hadoop.

    5.pyspark.sql, SQLContext, createOrReplaceTempView, dan show adalah komponen PySpark yang digunakan untuk memanipulasi dan menampilkan data dalam format DataFrame.

    6.textFile, map, lambda, strip, StructField, dan StringType adalah komponen PySpark yang digunakan untuk membaca dan memproses data teks.

    7.spark.read.format, jdbc, options, dan load adalah komponen PySpark yang digunakan untuk membaca data dari sumber eksternal seperti database.

    8.show adalah metode PySpark yang digunakan untuk menampilkan data dalam format DataFrame.

    9.collect, rdd, dan take adalah metode PySpark yang digunakan untuk mengambil data dari RDD dan menampilkannya dalam format yang sesuai.

    10.makeRDD, Seq, dan createDataset adalah metode PySpark yang digunakan untuk membuat RDD dan Dataset dari data yang diberikan.

    11.filter adalah metode PySpark yang digunakan untuk menyaring data dalam RDD atau DataFrame.

    12.as, toDF, dan first adalah metode PySpark yang digunakan untuk mengonversi data ke format DataFrame dan melakukan operasi di dalamnya.

    13.listDatabases, listTables, listFunctions, isCached, dan select adalah metode PySpark yang digunakan untuk memanipulasi dan mengambil data dari database dalam format DataFrame.

    14.read dan text adalah metode PySpark yang digunakan untuk membaca data teks.

    15.load, json, format, dan printSchema adalah metode PySpark yang digunakan untuk membaca dan menampilkan data dalam format JSON.

    16.write dan save adalah metode PySpark yang digunakan untuk menulis dan menyimpan data dalam format yang ditentukan.

    17.parquet adalah format file kolomar untuk menyimpan data di lingkungan Hadoop.
    
    18.Options, inferSchema, csv, header, dan codec adalah opsi dan metode yang dapat digunakan dalam PySpark untuk membaca, menulis, dan memanipulasi data dalam berbagai format.

# Hasil Praktikum
# accessingMetadata
![Screenshot](images/accessingMetadata.png)
# CreateDatasets
![Screenshot](images/CreateDatasets.png)
# CreatingDataFrames
![Screenshot](images/CreatingDataFrames.png)
# CreatingDataFramesfromDBs
![Screenshot](images/CreatingDataFramesfromDBs.png)
# CreatingDataFramesfromHive
![Screenshot](images/CreatingDataFramesfromHive.png)
# DataFrames2RDD
![Screenshot](images/DataFrames2RDD.png)
# Datasets2DataFramesViceVersa
![Screenshot](images/Datasets2DataFramesViceVersa.png)
# DataSourceAVRO
![Screenshot](images/DataSourceAVRO.png)
# DataSourceCSV
![Screenshot](images/DataSourceCSV.png)
# DataSourceJDBC
![Screenshot](images/DataSourceJDBC.png)
# DataSourceJson
![Screenshot](images/DataSourceJson.png)
# DataSourceORC
![Screenshot](images/DataSourceORC.png)
# DataSourceParquet
![Screenshot](images/DataSourceParquet.png)
# DataSourceTEXT
![Screenshot](images/DataSourceTEXT.png)
# DataSourceXML
![Screenshot](images/DataSourceXML.png)
# PandasDF2DataFrame
![Screenshot](images/PandasDF2DataFrame.png)
# SparkOnHBaseConnector
![Screenshot](images/SparkOnHBaseConnector.png)

##### Komang Gede Narariya Suputra
##### 2041720225
##### TI3B
##### 10
