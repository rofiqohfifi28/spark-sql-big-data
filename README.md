# spark-sql-big-data




## Praktikum 2

1.mylist , myschema : dua variabel dalam Python yang digunakan untuk membuat DataFrame di PySpark. mylist adalah daftar item, sedangkan myschema adalah daftar string yang mendefinisikan nama kolom DataFrame.


2.spark.createDataFrame : metode PySpark yang digunakan untuk membuat DataFrame dari data yang diberikan.

3.parallelize : metode PySpark yang digunakan untuk membuat RDD dari data yang diberikan, sedangkan toDF adalah metode yang digunakan untuk mengonversi RDD menjadi DataFrame.

4.hadoop, fs, put: perintah untuk mengunggah file ke sistem file Hadoop.

5.pyspark.sql, SQLContext, createOrReplaceTempView, dan show: komponen PySpark yang digunakan untuk memanipulasi dan menampilkan data dalam format DataFrame.

6.textFile, map, lambda, strip, StructField, dan StringType : komponen PySpark yang digunakan untuk membaca dan memproses data teks.

7.spark.read.format, jdbc, options, dan load : komponen PySpark yang digunakan untuk membaca data dari sumber eksternal seperti database.

8.show : metode PySpark yang digunakan untuk menampilkan data dalam format DataFrame.

9.collect, rdd, dan take : metode PySpark yang digunakan untuk mengambil data dari RDD dan menampilkannya dalam format yang sesuai.

10.makeRDD, Seq, dan createDataset : metode PySpark yang digunakan untuk membuat RDD dan Dataset dari data yang diberikan.

11.filter adalah : PySpark yang digunakan untuk menyaring data dalam RDD atau DataFrame.

12.as, toDF, dan first : metode PySpark yang digunakan untuk mengonversi data ke format DataFrame dan melakukan operasi di dalamnya.

13.listDatabases, listTables, listFunctions, isCached, dan select : metode PySpark yang digunakan untuk memanipulasi dan mengambil data dari database dalam format DataFrame.

14.read dan text : metode PySpark yang digunakan untuk membaca data teks.

15.load, json, format, dan printSchema : metode PySpark yang digunakan untuk membaca dan menampilkan data dalam format JSON.

16.write dan save : metode PySpark yang digunakan untuk menulis dan menyimpan data dalam format yang ditentukan.

17.parquet : format file kolomar untuk menyimpan data di lingkungan Hadoop.

18.Options, inferSchema, csv, header, dan codec : opsi dan metode yang dapat digunakan dalam PySpark untuk membaca, menulis, dan memanipulasi data dalam berbagai format.
